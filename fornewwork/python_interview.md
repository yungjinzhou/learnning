## 一、linux相关



### 1. 常用linux系统调用

系统调用（System Call）是操作系统为在用户态运行的进程与**硬件设备（如CPU、磁盘、打印机等）进行交互**提供的一组接口。当用户进程需要发生系统调用时，CPU 通过软中断切换到内核态开始执行内核系统调用函数

**系统调用在内核里的主要用途**。虽然给出了数种分类，不过总的概括来讲系统调用主要在系统中的用途无非以下几类：

- **控制硬件**——系统调用往往作为硬件资源和用户空间的抽象接口，比如读写文件时用到的write/read调用。

- **设置系统状态或读取内核数据**——因为系统调用是用户空间和内核的唯一通讯手段[2]，所以用户设置系统状态，比如开/关某项内核服务（设置某个内核变量），或读取内核数据都必须通过系统调用。比如getpgid、getpriority、setpriority、sethostname

- **进程管理**——一系列调用接口是用来保证系统中进程能以多任务，在虚拟内存环境下得以运行。比如 fork、clone、execve、exit等

**什么功能应该实现在内核而不是在用户空间**

- 服务必须获得内核数据，比如一些服务必须获得中断或系统时间等内核数据。

- 从安全角度考虑，在内核中提供的服务相比用户空间提供的毫无疑问更安全，很难被非法访问到。

- 从效率考虑，在内核实现服务避免了和用户空间来回传递数据以及保护现场等步骤，因此效率往往要比实现在用户空间高许多。比如,httpd等服务。

- 如果内核和用户空间都需要使用该服务，那么最好实现在内核空间，比如随机数产生。



信号：kill、signal/sigpending/sigsuspend/

管道：pipe

socket控制：socket/bind/connect/accept/send/listen/select/shutdown/setsockopt

用户管理：getuid/setuid/getgid/setgid/

网络管理：gethostname/sethostname/setdomainname/getdomainname/gethostid/sethostid

系统控制：reboot/time/uname/

文件系统控制：open/creat/close/read/write/readv/writev/pread/poll/truncate/access/stat/chown/chmod/chdir/rename/mkdir/mount/unmount/

进程控制：fork/clone/exit/execve/setpgid/getpid/getppid/nice/pause/ptrace/wait/wait3/waitpid/setsid/getsid

#### open

在用户态使用open()时，必须向该函数传入文件路径和打开权限。这两个参数传入内核后，内核首先检查这个文件路径存在的合法性，同时还需检查使用者是否有合法权限打开该文件。如果一切顺利，那么内核将对访问该文件的进程创建一个file结构。

在用户态，通常open()在操作成功时返回的是一个非负整数，即所谓的文件描述符（fd，file descriptor）；并且，用户态后续对文件的读写操作等都是通过fd来完成的。由此可见fd与file结构在内核中有一定的关联。

内核使用进程描述符task_struct来描述一个进程，而该进程所有已打开文件对应的file结构将形成一个数组files（其为files_struct结构），内核向用户返回的fd便是该数组中具体file结构的索引。默认情况下，每个进程创建后都已打开了标准输入文件、标准输出文件、标准错误文件，因此他们的文件描述符依次为0、1和2。



参考连接：https://www.cnblogs.com/shijiaqi1066/p/5749030.html





### 2. 进程、线程、协程、绿色线程



1. 进程：是一个正在执行的程序的实例。每个进程都有独立的内存空间，可以同时执行多个进程来完成不同的任务，进程是操作系统进行资源分配和调度的基本单位。
2. 线程：是进程中的一个执行路径，不同的线程可以同时执行在同一个进程内，共享该进程的内存资源。线程之间的切换开销比进程小，但线程需要考虑同步和互斥的问题。
3. 协程：是一种轻量级的线程，可以在单线程内实现多个任务的并发执行，也可以通过多进程来实现并发。协程之间的切换开销很小，通常使用非抢占式调度方式，需要手动控制任务的切换。



1. 进程之间通信需要IPC机制，线程之间通信可以使用共享内存，协程之间通信可以使用全局变量。不同的进程之间需要通过进程间通信（IPC）机制来进行数据交换；线程之间可以直接访问共享内存进行数据交换；协程之间可以使用全局变量等方式进行数据交换。
2. 进程切换开销较大，线程切换开销比进程小，协程切换开销最小。由于进程之间相互独立，因此进程切换的开销较大；线程切换的开销要比进程小，但需要考虑同步和互斥的问题；协程切换的开销最小，通常使用非抢占式调度方式，需要手动控制任务的切换。
3. 进程数受限于硬件资源，线程数受限于进程内存空间，协程数无限制。由于每个进程都有独立的内存空间，因此进程数量是受限于硬件资源的；线程共享进程内存空间，因此线程数量是受限于进程内存空间的，而协程数量没有限制。





绿色线程

通常使用第三方库（如gevent、eventlet等）来实现，因为Python标准库中没有原生支持绿色线程的机制。绿色线程使用的是替换调度器的方式，在一个线程内模拟多个线程之间的切换，实现异步非阻塞的目的。

协程

则是在Python标准库中有原生支持的概念，通常使用async/await语法结合asyncio库来实现。协程通过使用事件循环（Event Loop）和异步IO等机制来实现非阻塞的并发执行。



IPC（Inter-Process Communication，进程间通信）是指不同进程之间进行数据交换和协作的机制。在操作系统中，进程之间有时需要共享数据、资源或者通知对方做一些操作，这时就需要使用IPC机制。

常见的IPC机制包括：

1. 管道（Pipe）：是一种半双工的通信方式，可以实现父子进程或兄弟进程之间的通信。管道有两种，一种是匿名管道，只能用于父子进程间的内部通信；另一种是命名管道，可以在不同进程之间通信。
2. 信号量（Semaphore）：是一个计数器，用于多个进程之间的同步和互斥操作。通过对信号量的P操作和V操作来实现进程之间的同步和互斥。
3. 共享内存（Shared Memory）：是指多个进程共享同一块物理内存，并且可以相互访问。可以通过共享内存来提高进程间的通信效率。
4. 套接字（Socket）：是一种计算机之间网络通信的机制，不仅可以在同一台计算机上的进程之间通信，也可以在不同计算机之间通信。
5. 消息队列（Message Queue）：是一种存放消息的队列，被多个进程共享。每个消息都有一个类型，接收进程可以选择性地接收某一类型的消息。
6. 信号（Signal）：是异步通知的一种方式，可以在进程中发送软件中断信号，让接收进程进行相应的处理。

不同的IPC机制各有优劣，需要根据具体的应用场景来选择合适的机制。







### 3. poll/epoll/select



**不同操作系统中IO多路复用模型介绍**

 

#### 3.1 select机制

 IO多路复用模型得以实现得核心：就是操作系统 监控1个[sk......conn,]列表，不断轮询每1个sk/conn/是否可以accpet/revive，随着监控列表的增加，效率会递减；

select函数监视的文件描述符分为3类，分别是writefds、readfds和exceptfds。调用后select函数会被阻塞，直到有描述符就绪（有数据可读、可写或者有except）、或者超时(timeout可用于指定等待时间，如果想立即返回可设置为null)，函数返回。当select函数返回后，可以通过遍历fdset来找到就绪的描述符。

> 在网络编程中统一的操作顺序是创建socket－>绑定端口－>监听－>accept->write/read,当有客户端连接到来时,select会把该连接的文件描述符放到fd_set（一组文件描述符(fd)的集合）,然后select会循环遍历它所监测的fd_set内的所有文件描述符，当select循环遍历完所有fd_set内指定的文件描述符对应的poll函数后，如果没有一个资源可用(即没有一个文件可供操作)，则select让该进程睡眠，一直等到有资源可用为止，fd_set是一个类似于数组的数据结构，由于它每次都要遍历整个数组，所有她的效率会随着文件描述符的数量增多而明显的变慢，除此之外在每次遍历这些描述符之前，系统还需要把这些描述符集合从内核copy到用户空间，然后再copy回去，如果此时没有一个描述符有事件发生（例如：read和write）这些copy操作和便利操作都是无用功，可见slect随着连接数量的增多，效率大大降低。可见如果在高并发的场景下select并不适用，况且select默认的最大描述符为1024，如果想要更多还要做响应参数的配置。



![img](./select模型.png)



1. **最大限制**：单个进程能够监视的文件描述符的数量存在最大限制。(基于数组存储的赶脚)一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。它由FD_SETSIZE设置，32位机默认是1024个。64位机默认是2048.
2. **时间复杂度：** 对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低，时间复杂度O(n)。
   当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度，不管哪个Socket是活跃的，都遍历一遍。这会浪费很多CPU时间。
   它仅仅知道有I/O事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以**select具有O(n)的无差别轮询复杂度**，同时处理的流越多，无差别轮询时间就越长。
3. **内存拷贝：**需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大。



**支持操作系统**：linux/windows

 

#### 3.2 poll机制

不同于select使用三个位图来表示三个fdset的方式，poll使用一个pollfd指针来实现。pollfd结构包含了要监视的event和发生的event，不再使用select的“参数-值”传递的方式。同时pollfd并没有最大数量限制（但是数量过大后其性能也会降低）。和select函数一样，poll返回后需要轮询pollfd来获取就绪的描述符。

**没有最大连接数的限制**。（基于链表来存储的）

**支持操作系统**：linux

 

#### 3.3 epoll机制

##### epoll原理与流程

1.epoll很高级，epoll不会去再通过操作循环检查监控的socket列表中，那些socket出现了读操作，而是给需要监听的socket 1--1绑定1个回调函数；

2.检测的socket中 有1个soket出现了读操作，直接执行调用那个和该sk/con绑定的回调函数执行sk.accpet() 和conn.receve()

> ###### epoll基本流程
>
> **一棵红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。**
>
> 1. 执行 epoll_create
>    内核在epoll文件系统中建了个file结点，（使用完，必须调用close()关闭，否则导致fd被耗尽）
>    在内核cache里建了红黑树存储epoll_ctl传来的socket，
>    在内核cache里建了rdllist双向链表存储准备就绪的事件。
> 2. 执行 epoll_ctl
>    如果增加socket句柄，检查红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，告诉内核如果这个句柄的中断到了，就把它放到准备就绪list链表里。
>    ps：所有添加到epoll中的事件都会与设备（如网卡）驱动程序简历回调关系，相应的事件发生时，会调用回调方法。
> 3. 执行 epoll_wait
>    立刻返回准备就绪表里的数据即可（将内核cache里双向列表中存储的准备就绪的事件 复制到用户态内存）
>    当调用epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可。
>    如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户。



##### epoll基本特点

1. **边缘触发**，它只告诉进程哪些fd刚刚变为就绪态，并且只会通知一次。
2. **事件驱动，**每个事件关联上fd，使用事件就绪通知方式，通过 epoll_ctl 注册 fd，一旦该fd就绪，内核就会采用 callback 的回调机制来激活该fd，epoll_wait 便可以收到通知。

1. **没有最大连接数的限制**。（基于 红黑树+双链表 来存储的:1G的内存上能监听约10万个端口）
2. **时间复杂度低：** 边缘触发和事件驱动，监听回调，时间复杂度O(1)。
   只有活跃可用的fd才会调用callback函数；即epoll最大的优点就在于它只管“活跃”的连接，而跟连接总数无关，因此实际网络环境中，Epoll的效率就会远远高于select和poll。
3. **内存拷贝：**利用mmap()文件映射内存加速与内核空间的消息传递，减少拷贝开销。



##### epoll两种模式(LT/ET)

epoll对文件描述符的操作有两种模式：LT(level trigger) 和 ET(edge trigger)。LT是默认的模式，ET是“高速”模式。

- LT（水平触发）模式下，只要有数据就触发，缓冲区剩余未读尽的数据会导致 epoll_wait都会返回它的事件；
- ET（边缘触发）模式下，只有新数据到来才触发，不管缓存区中是否还有数据，缓冲区剩余未读尽的数据不会导致epoll_wait返回



##### 适合用epoll的应用场景：

- 对于连接特别多，活跃的连接特别少(大量的idle-connection)
- 典型的应用场景为一个需要处理上万的连接服务器，例如各种app的入口服务器，例如qq

##### 不适合epoll的场景：

- 连接比较少，数据量比较大，例如ssh (没有大量的idle-connection或者dead-connection)
  epoll 的惊群问题：
  因为epoll 多用于多个连接，只有少数活跃的场景，但是万一某一时刻，epoll 等的上千个文件描述符都就绪了，这时候epoll 要进行大量的I/O，此时压力太大。



Python中的selectors模块就是帮我们自动选择最佳IO多路复用代理的；



### 2.1.Event Loop的惊群效应

通过查阅资料发现`Linux`通过`WQ_FLAG_EXCLUSIVE`标记解决了`socket.accept`的惊群问题， 但是现在很多服务通过基于事件循环的方法来提供更高的并发能力。比如我线上运行的服务就是用到了**`Gevent`，而`Gevent`用到的核心事件循环则是`Epoll`**，它与`Select`, `Poll`并称为`Event Loop`。

对于任何工作模式来说， 使用`Event Loop`后，进程调用`socket.accept`后的行为逻辑就不一样了，具体的逻辑步骤如下：

- 1.进程在调用`socket.accept`时，`Event Loop`会把进程挂在`socket`对应的文件描述符的等待队列上。
- 2.当`socket`的文件描述符有事件产生时，对应的驱动就会将等待队列上对应的进程进行唤醒。
- 3.被唤醒的进程会通过`Event Loop`检查事件是否就绪，如果事件就绪就会返回对应的事件给刚才的进程。
- 4.检查`accept`事件是否可调用， 如果可以就执行`accept`操作，并取得该四元组的对应`socket`。

可以看到，之前进程是挂在网络驱动上等着被内核唤醒，而在使用`Event Loop`后进程是挂在对应文件描述符的等待队列上等待被`Event Loop`唤醒，对于`Pre-Worker`模型下的每个工作进程虽然都有自己专属的`Event Loop`，但是他们都是等待着同样的资源，于是当该文件描述符有事件产生时，就会唤醒所有工作进程对应的`Event Loop`来检查事件以及判断是否可以返回事件给工作进程, 而且由于是通过`Event Loop`的逻辑来执行`socket.accept`，这样会绕过上面所说的`WQ_FLAG_EXCLUSIVE`标记的限制，从而又产生了惊群效应。

可以看到，`Event Loop`产生惊群效应的原因跟进程直接调用`sock.accept`十分的像，所以他们的解决思路也很像，首先是给`Event Loop`增加一个名为`EPOLLEXCLUSIVE`的标记， 然后开发者在编程时可以在`Event Loop`实例化后注册对应的标记,当进程在调用`sock.accept`且系统检到`Event Loop`带有该标记时，就会把进程挂在文件描述符的队列尾部，等到事件产生时，**内核会只唤醒该队列的第一个进程来处理对应的事件。**

> 关于标记`EPOLLEXCLUSIVE`的具体内容可见:[Add epoll round robin wakeup mode](https://link.juejin.cn?target=https%3A%2F%2Flwn.net%2FArticles%2F632590%2F)， 通过内容还可以知道还有一个标记`EPOLLROUNDROBIN`用来解决唤醒不均衡的情况，但是在`Python`中似乎没办法使用。



☆☆☆参考连接：https://juejin.cn/post/7082005823328632839



### 4. pre-worker服务模型

包含nginx 、gunicorn/gevent/event loop/惊群现象及解决方法

tcp三种工作模式

负载不均衡问题

（gevent/**asyncio**，）

由于之前一直在使用`Asyncio`，所以我知道`Event Loop`在收到对应文件描述符的事件时，它不是以雨露均沾的方式去唤醒进程/线程/协程，而是**会优先唤醒第一个注册的进程/线程/协程，只有第一个进程/线程/协程繁忙的情况下才会去唤醒后面的进程/线程/协程，造成了唤醒倾斜的问题**，所以我猜测是这个规则引发了负载不均衡的问题。



☆☆☆参考连接：https://juejin.cn/post/7082005823328632839



### 5. nginx的惊群现象

首先，我们先大概梳理一下 Nginx 的网络架构，几个关键步骤为：

1. Nginx 主进程解析配置文件，根据 listen 指令，将监听套接字初始化到全局变量 ngx_cycle 的 listening 数组之中。此时，监听套接字的创建、绑定工作早已完成。
2. Nginx 主进程 fork 出多个子进程。
3. 每个子进程在 ngx_worker_process_init 方法里依次调用各个 Nginx 模块的 init_process 钩子，其中当然也包括 NGX_EVENT_MODULE 类型的 ngx_event_core_module 模块，其 init_process 钩子为 ngx_event_process_init。
4. ngx_event_process_init 函数会初始化 Nginx 内部的连接池，并把 ngx_cycle 里的监听套接字数组通过连接池来获得相应的表示连接的 ngx_connection_t 数据结构，这里关于 Nginx 的连接池先略过。我们主要看 ngx_event_process_init 函数所做的另一个工作：如果在配置文件里**没有**开启 [accept_mutex 锁](http://nginx.org/en/docs/ngx_core_module.html#accept_mutex)，就通过 ngx_add_event 将所有的监听套接字添加到 epoll 中。
5. 每一个 Nginx 子进程在执行完 ngx_worker_process_init 后，会在一个死循环中执行 ngx_process_events_and_timers，这就进入到事件处理的核心逻辑了。
6. 在 ngx_process_events_and_timers 中，如果在配置文件里开启了 accept_mutext 锁，子进程就会去获取 accet_mutext 锁。如果获取成功，则通过 ngx_enable_accept_events 将监听套接字添加到 epoll 中，否则，不会将监听套接字添加到 epoll 中，甚至有可能会调用 ngx_disable_accept_events 将监听套接字从 epoll 中删除（如果在之前的连接中，本worker子进程已经获得过accept_mutex锁)。
7. ngx_process_events_and_timers 继续调用 ngx_process_events，在这个函数里面阻塞调用 epoll_wait。

至此，关于 Nginx 如何处理 fork 后的监听套接字，我们已经差不多理清楚了，当然还有一些细节略过了，比如在每个 Nginx 在获取 accept_mutex 锁前，还会根据当前负载来判断是否参与 accept_mutex 锁的争夺。

把这个过程理清了之后，Nginx 解决惊群问题的方法也就出来了，就是利用 accept_mutex 这把锁。

**如果配置文件中没有开启 accept_mutex，则所有的监听套接字不管三七二十一，都加入到每子个进程的 epoll 中，这样当一个新的连接来到时，所有的 worker 子进程都会惊醒。**

**如果配置文件中开启了 accept_mutex，则只有一个子进程会将监听套接字添加到 epoll 中，这样当一个新的连接来到时，当然就只有一个 worker 子进程会被唤醒了。**



### 6. ifconfig参数解释

```
ifconfig
ens33: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500
inet 192.168.1.63 netmask 255.255.255.0 broadcast 192.168.1.255
inet6 fe80::c09d:975d:89cd:fd3f prefixlen 64 scopeid 0x20
ether 00:0c:29:02:83:db txqueuelen 1000 (Ethernet)
RX packets 3255 bytes 4458479 (4.2 MiB)
RX errors 0 dropped 26 overruns 0 frame 0
TX packets 1130 bytes 81645 (79.7 KiB)
TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0

上图信息大概说明：
第一行：up–>网卡开启状态
RUNNING–>网线处理连接状态
MULTICAST–>支持组播
mtu 1500–>（Maximum Transmission Unit）最大传输单元大小为1500字节
第二行：该网卡的IP地址，子网掩码，广播地址
第三行：IPV6的配置信息
第四行：网卡的MAC地址
ether表示连接类型为以太网
txqueuelen 1000 --》传输队列的长度
第五六行：网卡接收数据包的统计信息和接收错误的统计信息
第七八行：网卡发送数据包的统计信息和发送错误的统计信息
```

















常见进程调度算法





操作系统如何申请及管理内存





同步、阻塞、异步、并发、非阻塞、并行



nginx惊群现象 







## 二、网络相关





### 2.1. 三次握手、四次挥手



TCP三次握手是建立一个TCP连接时使用的一种可靠的握手过程。以下是TCP三次握手的步骤：

1. 第一次握手（SYN）：客户端向服务器发送一个带有SYN（同步）标志的数据包，表明客户端请求建立连接。这个数据包包含客户端随机生成的初始序列号（ISN）。
2. 第二次握手（SYN-ACK）：服务器收到客户端的SYN数据包后，如果同意建立连接，则会向客户端发送一个带有SYN和ACK（确认）标志的数据包。该数据包中，SYN标志表示服务器已经收到了客户端的请求，ACK标志表示服务器确认了客户端的初始序列号，并且服务器也随机生成了自己的初始序列号。
3. 第三次握手（ACK）：客户端收到服务器的SYN-ACK数据包后，会向服务器发送一个带有ACK标志的数据包作为确认，表明客户端已经收到了服务器的确认，并且连接已经建立。

完成了三次握手后，TCP连接就建立起来了，双方可以开始进行数据传输了。

TCP三次握手的目的是确保客户端和服务器都能够相互发送和接收数据，并且双方都知道对方可靠地收到了数据。通过这个握手过程，双方能够确认彼此的初始序列号，并同步初始化各自的TCP缓冲区，建立可靠的连接。

需要注意的是，在网络中存在一些常见的问题，比如网络延迟、丢包等，可能导致握手过程的延迟或失败。为了解决这些问题，TCP实现中通常设置了一些超时和重传机制，确保握手过程的可靠性和稳定性。



TCP四次挥手是用于关闭一个已建立的TCP连接的过程。以下是TCP四次挥手的步骤：

1. 第一次挥手（FIN）：当一方（通常是客户端）需要关闭连接时，发送一个带有FIN（终止）标志的数据包给另一方（通常是服务器），表示自己不再发送数据，但仍然可以接收数据。
2. 第二次挥手（ACK）：接收到关闭请求的一方（通常是服务器）对此发出确认，发送一个带有ACK（确认）标志的数据包给请求关闭的一方（通常是客户端），表示已收到关闭请求。
3. 第三次挥手（FIN）：关闭请求的一方（通常是服务器）同样也需要关闭连接，发送一个带有FIN标志的数据包给另一方（通常是客户端），表示自己不再发送数据。
4. 第四次挥手（ACK）：接收到关闭请求的一方（通常是客户端）对此发出确认，发送一个带有ACK标志的数据包给请求关闭的一方（通常是服务器），表示已收到关闭请求。

完成四次挥手后，TCP连接就被正常关闭了。这个过程确保了双方都能够停止发送数据，并且双方都知道对方已经关闭了连接。

需要注意的是，在挥手过程中可能会遇到延迟、丢包等网络问题，因此TCP实现中通常会设置一些超时和重传机制，以确保挥手过程的可靠性和稳定性。

另外，值得注意的是，关闭连接后，被动关闭的一方（通常是服务器）可能还需要一段时间来处理未完全接收的数据，这被称为"TIME_WAIT"状态。该状态的持续时间取决于实现，通常为几分钟，目的是确保在网络中所有延迟的或重复的分组都可以被丢弃，从而不会干扰到下一个连接的建立。







为什么是三次握手、不是两次握手，

参考-learnning\jike_qutanwangluoxieyi/网络学习





### http与https



HTTP（Hypertext Transfer Protocol）和 HTTPS（Hypertext Transfer Protocol Secure）是用于在客户端和服务器之间传输数据的协议。

1. HTTP：HTTP是一种无状态的协议，用于在Web浏览器和Web服务器之间传输超文本数据。它使用明文传输数据，不提供数据加密和安全性保护。HTTP的默认端口是80，通过URL以"http://"开头来访问。HTTP适用于一些不涉及敏感信息的普通网页浏览、数据传输等场景。
2. HTTPS：HTTPS是基于HTTP的安全协议，使用SSL/TLS协议对传输的数据进行加密和身份验证。它通过在HTTP和TCP之间加入SSL/TLS层来实现数据加密和安全性保护。HTTPS的默认端口是443，通过URL以"https://"开头来访问。HTTPS适用于需要保护用户隐私、进行在线支付、登录账户等安全敏感操作的场景。

区别：

- 安全性：HTTPS通过加密传输数据，保护敏感信息免受窃听和篡改。而HTTP传输的数据是明文的，容易被拦截和篡改。
- 证书：HTTPS使用SSL/TLS证书进行身份验证，确保客户端与服务器的通信是可信的。而HTTP没有使用证书进行认证，可能存在中间人攻击的风险。
- 端口：HTTP的默认端口是80，而HTTPS的默认端口是443，用于区分两种协议。

选择使用HTTP还是HTTPS取决于具体的应用场景。对于需要保护数据安全和用户隐私的场景，特别是涉及敏感信息传输的情况下，应优先选择使用HTTPS。



HTTP（Hypertext Transfer Protocol）是一种用于传输超媒体文档（如 HTML）的应用层协议。它是基于客户端-服务器模型的，通过请求-响应的方式进行通信。下面是关于 HTTP 协议的一些重要信息：

1. 请求和响应：HTTP 协议中的通信是通过请求和响应来完成的。客户端发送一个 HTTP 请求到服务器，并等待服务器返回一个 HTTP 响应。
2. URL（Uniform Resource Locator）：URL 是用于标识资源在互联网上位置的地址。在 HTTP 请求中，客户端通过 URL 来指定要请求的资源。
3. 方法（Method）：HTTP 定义了多种请求方法，其中最常见的是 GET 和 POST。GET 方法用于获取资源，而 POST 方法用于提交数据到服务器。
4. 状态码（Status Code）：HTTP 响应中包含一个状态码，用于表示服务器对请求的处理结果。常见的状态码有 200（成功）、404（未找到）、500（服务器内部错误）等。
5. 头部（Headers）：HTTP 请求和响应中都包含头部信息，用于传递额外的元数据。头部可以包含各种信息，如身份认证、内容类型、缓存控制等。
6. 实体主体（Entity Body）：在一些请求或响应中，可以包含一个实体主体，用于携带数据或资源。例如，在 POST 请求中，实体主体通常包含要提交的表单数据。
7. Cookie：Cookie 是一种在客户端存储数据的机制，用于在不同的请求之间保持会话状态。服务器可以通过 Set-Cookie 头部将一个 Cookie 发送给客户端，客户端在后续的请求中将该 Cookie 包含在 Cookie 头部中发送回服务器。
8. 缓存：HTTP 支持缓存机制，可以减少对服务器资源的请求。服务器可以通过 Cache-Control 头部指定缓存策略，例如缓存时间、是否允许缓存等。
9. 安全性：HTTP 可以通过使用 HTTPS（HTTP Secure）来提供安全的通信。HTTPS 使用 SSL/TLS 加密协议对通信进行加密，以保护敏感信息的安全性。
10. 无状态协议：HTTP 是一种无状态协议，即服务器不会保留之前请求的信息。每次请求都是独立的，服务器不能识别不同请求之间的关联性，因此需要使用 Cookie 或其他方式来维护会话状态。

HTTP 协议在互联网中被广泛使用，它是 Web 应用程序通信的基础。通过了解和使用 HTTP，开发人员可以构建出高效、可靠的网络应用程序。



HTTPS（安全套接层超文本传输协议）提供了一种加密和身份验证的机制，以保证在网络中进行的数据传输的机密性和完整性。下面是HTTPS安全性的几个关键方面：

1. 数据加密：HTTPS使用传输层安全协议（TLS）或其前身-安全套接层（SSL）来对传输的数据进行加密。这意味着在数据从客户端发送到服务器的过程中，第三方无法轻易获取到明文数据。加密确保敏感信息（如登录凭据、支付详情等）在传输过程中不被窃取或篡改。
2. 身份验证：HTTPS使用数字证书来对服务器进行身份验证。证书由可信的第三方机构（称为证书颁发机构）颁发，用于证明网站的真实性和合法性。通过检查所收到的证书，用户可以确认他们连接的是合法的服务器，而不是恶意的伪装网站。
3. 安全连接建立：HTTPS在建立连接时使用握手协议来进行密钥的交换和身份验证。这个过程中，客户端和服务器会相互验证证书，并生成一个用于加密通信的共享密钥。这确保了双方之间的通信不会被窃听、篡改或伪造。
4. 完整性保护：HTTPS使用消息认证码（MAC）来验证传输的数据是否在传输过程中被篡改。当数据经过加密后，MAC会使用共享密钥对数据进行签名，接收方可以使用相同的密钥来验证签名，确保数据的完整性。
5. 浏览器安全标识：HTTPS连接会在浏览器地址栏显示一个锁形状的安全标识，并在访问使用HTTP的网站时给出警告。这帮助用户识别哪些网站是通过安全的HTTPS连接进行通信的，从而提高用户对网站的信任度。

综上所述，HTTPS通过数据加密、身份验证、安全连接建立、完整性保护和浏览器安全标识等机制，提供了更安全的数据传输方式，以保护用户的隐私和数据安全。



### 0.0.0.0地址

是的，"0.0.0.0" 是一个合法的 IP 地址，它通常被用作一个通配符地址。在网络编程中，"0.0.0.0" 表示所有可能的 IP 地址或任何可用的网络接口。

具体而言，"0.0.0.0" 被称为“未指定地址”或“通配地址”。当服务器程序绑定到 "0.0.0.0" 时，表示该服务器会监听所有可用的网络接口上的请求，使得可以通过任意可用的 IP 地址访问该服务器。

使用 "0.0.0.0" 作为目标地址时，数据包会被路由到本地网络中的某个特定主机或者广播到本地网络上的所有主机，这取决于具体的网络配置和路由规则。

需要注意的是，虽然 "0.0.0.0" 是一个合法的 IP 地址，但一般情况下并不用作主机的实际 IP 地址。它更多地被用于网络编程、服务器配置和网络设备的配置等方面。





tcp与udp特点，区别



http网页，从请求到响应。 都走了那些步骤、（dns）

如果设置代理(外网代理)，请求有何不同





dns



http keepalive 和 tcp  keepalive





syn攻击、半连接



进程间如何通信



python的底层网络交互模块有哪些



OSI七层协议



## 三、数据库相关

### mysql



mysql引擎，各个引擎之间有什么区别



数据库事务，及其特性



数据库事务隔离级别有哪些、区别与特点



死锁发生的情况，如何解决



索引的原理

mysql  B+索引、优缺点

mysql索引类型

聚簇索引和非聚簇索引

唯一索引和普通索引区别，使用索引有哪些优缺点

myql索引什么情况下会失效





mysql主从同步机制



数据库的ACID



如何开启慢查询日志

#### sql查询慢如何分析



当SQL查询变慢时，可以采取以下步骤来定位问题：

1. 确认问题：首先确定SQL查询确实存在性能问题，并与预期结果相比较。可以使用数据库性能监控工具或日志来分析查询的执行时间和资源消耗。
2. 检查索引：检查查询所涉及的表是否有适当的索引。确保索引覆盖了查询条件和连接条件，并且统计信息是最新的。缺少或无效的索引可能导致全表扫描或索引失效，从而导致查询变慢。
3. 分析执行计划：使用数据库的执行计划工具，例如EXPLAIN或执行计划分析器，来获取查询的执行计划。分析执行计划可以帮助您理解查询是如何执行的，以及是否存在潜在的性能瓶颈，如全表扫描、排序操作、连接操作等。
4. 优化查询语句：检查查询语句是否能够进行优化。可能通过重写查询语句、调整连接顺序、使用更有效的操作符或子查询等方式来改进查询性能。
5. 监控资源利用：查看查询执行期间的系统资源利用情况，例如CPU使用率、内存消耗、磁盘I/O等。如果系统资源过载或瓶颈，可能会导致查询变慢。可以使用性能监控工具来检查系统资源的使用情况。
6. 数据库服务器配置：检查数据库服务器的配置参数是否合理，例如内存分配、并发连接数、查询缓存等。调整这些参数可能有助于提升查询性能。
7. 慢查询日志：开启数据库的慢查询日志，并分析其中记录的查询语句。慢查询日志可以帮助您识别最耗时的查询，并找出需要优化的部分。
8. 数据库统计信息：确保数据库的统计信息是最新的。统计信息用于查询优化器生成执行计划，如果统计信息不准确或过期，可能导致选择不正确的执行计划。

通过以上步骤，您可以逐步定位和解决SQL查询性能问题。在每个步骤中，可以使用相应的工具、日志和监控来帮助您进行分析和优化。



#### 索引失效

索引在某些情况下可能会失效，导致查询性能下降。以下是一些常见的索引失效情况：

1. 非选择性索引：如果索引的选择性很低，即索引列上具有相同或接近相同的值，那么数据库优化器可能会认为使用全表扫描比使用索引更高效。这种情况下，索引就会失效。
2. 不匹配的查询条件：当查询中的条件与索引定义不匹配时，索引也会失效。例如，如果索引是在字段A上创建的，但查询中使用了字段B的条件，那么索引将无法使用。
3. 隐式数据类型转换：如果查询中涉及到的字段类型与索引列的数据类型不同，数据库可能会进行隐式的数据类型转换。在这种情况下，索引也可能会失效，因为对索引列进行数据类型转换后无法使用索引。
4. 函数或表达式的使用：如果查询中使用了函数、表达式或计算，这可能导致索引失效。因为数据库引擎通常无法直接使用索引来评估复杂的函数或表达式。
5. OR条件的使用：对于含有OR条件的查询，如果其中一个条件无法使用索引，那么整个查询可能无法使用索引。
6. 排序和分组：如果查询需要进行排序或分组操作，但查询中的索引不涵盖排序或分组的字段，那么索引可能会失效。
7. 更新和删除操作：当对索引列进行大量的更新或删除操作时，索引可能会失效，因为数据库引擎需要维护索引的一致性并更新索引结构。

为了避免索引失效，建议在设计索引时考虑查询的常见模式和条件，并选择合适的索引策略。此外，定期检查和维护索引的统计信息也是保持索引性能的重要步骤。



#### B树和B+树的区别

B树和B+树是两种常用的索引结构，常用于数据库和文件系统中。它们在以下几个方面存在区别：

1. 节点结构：B树的节点中既包含键值，也包含对应的数据，因此一个节点可以同时存储键值和数据。而B+树的节点只包含键值，数据则仅存储在叶子节点中。
2. 叶子节点：在B树中，叶子节点存储了所有的键值和对应的数据。而B+树的叶子节点只包含键值和对应的数据，且叶子节点通过指针连接成链表，便于范围查询和遍历。
3. 键值的查找：在B树中，可以通过非叶子节点进行键值的查找，因为非叶子节点中包含了键值和对应的数据。而B+树的查找必须从根节点开始，经过非叶子节点，最终在叶子节点中找到目标键值。
4. 排序性：B树中的键值有序存储在节点中，因此范围查询时无需遍历叶子节点。而B+树的叶子节点通过指针连接成链表，并按键值大小顺序排序，适合范围查询。
5. 索引重建：在B树中，当节点的键值或数据发生变化时，需要对整棵树进行重建调整。而B+树的重建只需考虑叶子节点，非叶子节点不受影响，因此重建操作开销相对较小。
6. 应用场景：由于B+树的特性，适合大规模数据存储和范围查询，常用于数据库索引。而B树适用于存储磁盘块号的索引，如文件系统中的索引。

总体而言，B树和B+树在节点结构、叶子节点、键值查找、排序性、索引重建和应用场景等方面存在差异。根据实际需求选择适合的索引结构有助于提高查询性能和存储效率。



数据库的脏读、幻读、幻行的原理、发生场景，及解决方式





serializable 序列化、最好的事务级别



#### 乐观锁、悲观锁

悲观锁, 每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。

乐观锁，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制，乐观锁适用于多读的应用类型，这样可以提高吞吐量。





#### sql注入原理，

SQL注入（SQL Injection）是一种常见的Web应用程序安全漏洞，攻击者通过对Web应用程序的输入参数或其他可控制的变量注入恶意的SQL代码，从而达到非法获取、篡改、删除敏感数据等目的。

具体来说，SQL注入是指攻击者在提交的请求中，通过构造特定的字符串，欺骗服务器，使其将命令当作SQL语句来执行。比如，攻击者可以通过在表单中输入特殊字符，从而改变SQL查询的含义，使查询返回所有用户信息，而不仅仅是当前用户的信息。

```
如果用户搜索字符串为 Tom，则在后台生成的SQL查询语句为：
SELECT * FROM users WHERE name = 'Tom'
然而，如果黑客输入字符串：' or '1'='1 这就是一个典型的SQL注入攻击，这个字符串会被拼接到原始的SQL查询中，构造出下面的恶意查询语句：
SELECT * FROM users WHERE name = '' or '1'='1'
```

在 Python 中使用特殊的函数将用户输入值进行转义或对用户输入值中的引号等进行转义作为字符串而不是符号达到要求，可以使用一些库的方法，如 MySQLdb 库的 `escape_string()` 方法或 psycopg2 库的 `sql.Identifier()` 和 `sql.Literal()` 方法。





#### CAS

CAS（Compare and Set）操作是一种并发控制机制，常用于多线程或分布式系统中，用于确保在读取和修改共享资源之间的原子性操作。CAS操作包括三个步骤：比较、交换和判断。

1. 比较：首先，系统会比较共享资源的当前值与预期值是否相等。
2. 交换：如果比较结果相等，则将共享资源的值替换为新的值，否则不进行任何操作。
3. 判断：无论是否执行了交换操作，最后都会返回当前共享资源的值。

CAS操作具有原子性，因为它在执行时会先检查共享资源的当前值是否符合预期，如果符合预期则进行更新，否则不更新。这样可以避免多个线程同时修改同一共享资源导致的数据不一致问题。

在分布式系统中，CAS操作也可以用于实现乐观锁机制。每个节点在修改共享资源之前，先检查共享资源的版本号或时间戳，并将自己的修改操作与当前版本进行比较。如果一致，则进行修改，否则认为版本冲突并进行相应的处理。

在编程中，CAS操作是一种低级别的原子操作，可以通过硬件指令或特定的API来支持。在Java中，比较常见的CAS操作是利用`java.util.concurrent.atomic`包下的原子类，如`AtomicInteger`、`AtomicLong`等。

需要注意的是，CAS操作并不适用于所有场景，特别是在高并发环境下，可能存在ABA问题（即共享资源被修改两次，但比较时值恰好相等），需要结合具体情况进行评估和使用。



数据库的优化













### 3.2 redis

#### 3.2.1 redis单线程

Redis采用单线程，那么它是如何处理多个客户端连接请求呢？

Linux 中的 IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听套接字和已连接套接字。内核会一直监听这些套接字上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。

下图就是基于多路复用的 Redis IO 模型。图中的多个 FD 就是刚才所说的多个套接字。Redis 网络框架调用 epoll 机制，让内核监听这些套接字。此时，Redis 线程不会阻塞在某一个特定的监听或已连接套接字上，也就是说，不会阻塞在某一个特定的客户端请求处理上。正因为此，Redis 可以同时和多个客户端连接并处理请求，从而提升并发性。

![img](./redis-io.png)

参考链接：https://www.cnblogs.com/databasepub/p/16691115.html



Redis使用单线程模型，没有了线程上下文切换和访问共享资源加锁的性能损耗，配合IO多路复用技术，可以完成多个连接的请求处理。而且正是由于它的使用定位是内存数据库，这样几乎所有的操作都在内存中完成，它的性能可以达到非常之高。

**Redis 6.0 版本为什么又引入了多线程，这里也解释下。**

Redis 的性能瓶颈不在 CPU ，而在内存和网络，内存不够可以增加内存或通过数据结构等进行优化；但 Redis 的网络 IO 的读写占用了大部分 CPU 的时间，如果可以把网络处理改成多线程的方式，性能会有很大提升。所以总结下 Redis 6.0 版本引入多线程有两个原因：1.充分利用服务器的多核资源 2.多线程分摊 Redis 同步 IO 读写负荷

**注意：执行命令还是由单线程顺序执行，只是处理网络数据读写采用了多线程，而且 IO 线程要么同时读 Socket ，要么同时写 Socket ，不会同时读写。**





#### 数据结构



Redis（Remote Dictionary Server）是一种高性能键值存储系统，支持多种数据结构。下面是 Redis 中常用的数据结构：

1. 字符串（String）：在 Redis 中，字符串是最基本的数据结构。它可以存储任意类型的二进制数据，例如文本、JSON 数据等。字符串还支持一些操作，如设置和获取值、计数器操作、对字符串进行拼接等。
2. 哈希（Hash）：哈希是一个键值对的集合，类似于字典或关联数组。在 Redis 中，哈希适用于存储和操作对象。每个哈希可以包含多个字段，每个字段都有一个对应的值。哈希支持设置和获取单个字段的值，以及获取所有字段的值等操作。
3. 列表（List）：列表是一个有序的字符串集合，可以在列表的头部或尾部添加、删除元素。在 Redis 中，列表可以用于实现队列、栈等数据结构。列表还提供了按索引访问元素、获取子列表、插入和移除元素等操作。
4. 集合（Set）：集合是一组唯一且无序的元素的集合。在 Redis 中，集合可以用于存储和操作无序的标签、标识符等数据。集合提供了判断元素是否存在、添加和删除元素、求交集、并集、差集等操作。
5. 有序集合（Sorted Set）：有序集合是一个有序的字符串集合，每个元素都关联一个分数，用于排序。在 Redis 中，有序集合可以用于存储和操作排行榜、计分系统等数据。有序集合支持添加和删除元素，按分数范围获取元素，以及求交集、并集、差集等操作。
6. 地理空间索引（Geospatial Index）：Redis 还提供了地理空间索引的数据结构，通过经度和纬度来存储和查询位置信息。这使得 Redis 在地理空间应用中具有很强的功能，例如查找附近的商家、计算距离等。

Redis 的数据结构非常灵活和高效，可以根据应用的需求选择适合的数据结构。开发人员可以利用这些数据结构构建各种复杂的应用程序。



#### 缓存雪崩、缓存击穿、

缓存雪崩和缓存击穿是两个与缓存相关的问题，它们会导致系统性能下降甚至崩溃。

1. 缓存雪崩（Cache Avalanche）：
   - 定义：缓存雪崩指的是在某个时间点，大量缓存同时失效（或过期），导致大量的请求直接访问后端数据库，造成数据库压力过大或系统崩溃。
   - 原因：缓存雪崩通常是由于多个缓存键的过期时间相同或非常接近，并且在同一时间同时失效。
   - 解决方案：为了避免缓存雪崩，可以采取以下措施：
     - 为每个缓存键设置随机的过期时间，避免同一时间大量缓存失效。
     - 使用分布式锁或互斥锁来保证只有一个线程能够重新生成缓存，避免多个线程同时重建缓存。
     - 实施多级缓存架构，将热门数据缓存在内存中的Redis等作为一级缓存，将持久化存储（如数据库）作为二级缓存，确保即使一级缓存失效，仍有备用数据可供使用。
2. 缓存击穿（Cache Miss）：
   - 定义：缓存击穿指的是某个缓存键失效或不存在，并发请求同时访问该缓存键，导致大量请求绕过缓存直接访问后端数据库，造成数据库压力过大或系统崩溃。
   - 原因：缓存击穿通常是由于热点数据的缓存失效，而此时有大量并发请求同时访问该热点数据。
   - 解决方案：为了避免缓存击穿，可以采取以下措施：
     - 使用互斥锁（Mutex Lock）或分布式锁，在缓存失效时，只允许一个线程去加载数据库中的数据。
     - 缓存空对象（Cache Null Object），即使数据库查询为空，也将空结果缓存一段时间，避免直接查询数据库，减轻数据库负载。
     - 预先加载缓存，例如在系统启动或低峰期，预先将热门数据加载到缓存中，以减少缓存失效时的访问压力。
     - 使用限流和熔断机制来控制并发请求量，防止过多请求压垮系统。

缓存雪崩和缓存击穿都是需要综合考虑多种策略来解决的问题，具体的解决方案需要根据业务场景和需求进行选择和实施。





#### 分布式锁

Redis可以被用作分布式锁的实现，以下是使用Redis实现分布式锁的一般步骤：

1. 获取锁：客户端尝试在Redis中创建一个特定的键（即锁），可以使用`SETNX`命令（如果键不存在则设置键值对）来实现。通过设置合适的过期时间，确保即使锁未被显式释放，也能在一段时间后自动过期。另外，为了确保多个客户端同时争抢同一个锁时不会导致死锁，可以为每个客户端生成一个唯一的标识符，将其作为锁的值存储。
2. 判断锁是否获取成功：使用`SETNX`命令返回的结果来判断锁是否获取成功。如果返回1，则表示锁已经获取成功；如果返回0，则表示锁已经被其他客户端占用，当前客户端需要等待或执行其他逻辑。
3. 释放锁：当需要释放锁时，客户端可以使用`DEL`命令将锁从Redis中删除，以释放资源。为了确保只有锁的拥有者才能释放锁，可以将锁的值与客户端标识符进行比对，只有匹配时才执行删除操作。

需要注意的是，在使用Redis实现分布式锁时可能会面临一些挑战，例如：

- 死锁：如果某个持有锁的客户端崩溃或由于其他原因没有正确释放锁，可能会导致其他客户端无法获取锁而进入死锁状态。为了解决这个问题，可以使用适当的超时机制来确保即使锁的持有者无法显式释放锁，也能在一定时间内自动释放。
- 锁的超时：为了避免某个客户端在执行时间过长时占用锁资源，可以为锁设置一个合理的超时时间。如果某个客户端未能在规定时间内完成任务，锁会自动过期，其他客户端有机会获取该锁。
- 高并发场景下的竞争：在高并发环境中，多个客户端同时请求获取锁时可能引发竞争。可以考虑使用分布式锁算法（如Redlock、Mutex等）或者利用Redis提供的lua脚本来实现更复杂的锁逻辑，以确保在竞争情况下仅有一个客户端能够成功获取锁。

综上所述，Redis作为一个高性能的键值存储数据库，提供了一种简单而有效的方式来实现分布式锁，但在使用过程中需要注意以上挑战，并结合具体场景选择适合的实现方式。





#### redis和mysql底层实现



Redis和MySQL是两种不同类型的数据库，它们的底层实现原理和架构也有所不同。

Redis的底层实现采用了基于内存的键值存储结构。它使用一个单线程的事件循环模型来处理客户端请求，并通过异步I/O和非阻塞的方式与客户端进行通信。Redis将数据存储在内存中，因此具有快速的读写性能。为了保证数据的持久性，Redis可以将数据异步地写入磁盘上的持久化文件（RDB文件）或者通过写日志（AOF日志）的方式记录每个写操作，以便在重启后恢复数据。

MySQL则采用了传统的关系型数据库的架构，使用了多线程模型。它将数据以表格的形式存储在磁盘上，通过索引等技术来提高查询性能。MySQL支持事务，具有强大的ACID特性，适用于需要严格数据一致性和复杂查询的场景。MySQL还支持多种存储引擎，如InnoDB、MyISAM等，不同的存储引擎有着不同的实现方式和特点。

总结来说，Redis是一种基于内存的键值存储系统，适用于高速读写和对性能要求较高的场景，常用作缓存、消息队列等。而MySQL是一种传统的关系型数据库，适用于需要复杂查询和严格数据一致性要求的场景，常用于存储结构化数据。它们的底层实现原理和架构设计都是为了满足不同的使用需求和性能要求。



##### 有序集合

有序集合（Sorted Set）是Redis中的一种数据结构，它类似于集合（Set），但每个元素都会关联一个分数（score）。这个分数用来对集合中的元素进行排序，并且确保集合中的元素是唯一的。

有序集合在Redis中使用跳跃表（Skip List）和哈希表（Hash Table）两种数据结构实现。跳跃表用于提供元素的有序性，并支持快速的插入、删除和查找操作。而哈希表则用于存储元素与分数的映射关系，以便根据分数进行排序。

有序集合可以进行以下操作：

1. 添加元素：添加一个带有分数的元素到有序集合中，如果元素已经存在，则更新它的分数。
2. 删除元素：根据元素的值从有序集合中删除一个或多个元素。
3. 获取元素：根据元素的排名（从小到大）或者分数的范围，获取有序集合中的一个或多个元素。
4. 获取排名：根据元素的值，获取元素在有序集合中的排名（从小到大）。
5. 获取分数：根据元素的值，获取元素在有序集合中的分数。
6. 增减分数：给有序集合中的一个元素增加或减少分数。

有序集合的应用场景非常广泛。它可以用于排行榜、范围查找、任务调度等需要排序和按照分数进行筛选的场景。Redis提供了丰富的有序集合操作命令，方便对有序集合进行操作和利用其特性。  







redis 持久化中rdb和 aof方案的优缺点











## 四、python相关



### 交集差集并集

```
python
list1 = [1, 2, 3, 4, 5]
list2 = [4, 5, 6, 7, 8]


# 求交集
set1 = set(list1)
set2 = set(list2)
intersection = set1 & set2  # 或者使用 set1.intersection(set2)
print(intersection)  # 输出 {4, 5}


# 求差集
difference = set1 - set2  # 或者使用 set1.difference(set2)
print(difference)  # 输出 {1, 2, 3}


# 求对称差集
result = set1 ^ set2
print(result)  # {1, 2, 3, 6, 7, 8}
# 使用 symmetric_difference() 方法
# result = set1.symmetric_difference(set2)


# 求并集
union = set1 | set2  # 或者使用 set1.union(set2)
print(union)  # 输出 {1, 2, 3, 4, 5, 6, 7, 8}
```



### python垃圾回收机制

Python 的垃圾回收机制主要是基于引用计数和循环垃圾收集两种方式。

#### 引用计数

 在 Python 中，每个对象都有一个整数计数器，表示该对象的引用数。当对象被引用时，引用数加1；当引用被删除时，引用数减1。当引用数归零时，Python 会自动回收这个对象的内存空间。



引用的含义：在 Python 中，引用是一个指向对象的指针。当您创建一个变量并将其赋值给某个对象时，实际上是创建了一个指针，该指针指向该对象的内存位置。通过在变量中保留对该内存位置的引用，您可以在程序的其他部分使用对象。

python垃圾回收主要以引用计数为主，标记-清除和分代清除为辅的机制，其中标记-清除和分代回收主要是为了处理循环引用的难题。

#### 循环垃圾收集

引用计数虽然简单高效，但并不能完全解决内存泄漏的问题。

两个对象相互引用的情况是一种循环引用（Circular Reference）的情形。在这种情况下，每个对象的引用计数都不为零，因为它们相互引用对方，无法被垃圾回收机制回收。

为了解决这个问题，Python 还提供了一种循环垃圾收集机制。

标记-清除（Mark and Sweep）和分代清除（Generational Collection）都是常见的垃圾回收算法，用于在编程语言中自动释放内存空间，保证程序的稳定性和可靠性。

标记-清除算法通过遍历对象图，标记所有活动对象，并清除所有未被标记的垃圾对象。该算法消耗的时间较长，但可以处理任意形式的内存分配，适合于垃圾散布比较均匀的情况。缺点是在清除垃圾后可能会产生大量的碎片，导致内存利用率降低。

分代清除算法则将内存分成几个不同的区域，根据对象的存活时间和垃圾回收的频度将其分为不同的代。通常，只有在经过多次垃圾回收仍然存活的对象才会升级到更高的代。这样，对于不同代的对象，采取不同的垃圾回收策略，可以极大地提高垃圾回收效率。例如，对于生命周期较短的对象，可以采用标记-清除算法进行回收；对于生命周期较长的对象，则通过移动、整理等方式进行回收。该算法相对于标记-清除算法来说，具有更好的性能和空间利用率。

在 Python 中，所有的对象都有一个内部计数器，记录着当前被多少个变量引用着。在循环垃圾回收机制中，它会从一些特殊的地方开始扫描对象，这些地方称为“根”。比如，全局变量、当前执行代码的函数等都是根。然后，从这些根开始，回收机制会递归地访问所有能够访问到的对象，将它们标记为活动对象，也就是正在使用中的对象。而对于那些没有被标记的对象，则说明它们不再被使用，可以被清理掉。

在遍历对象图的过程中，当循环垃圾回收机制发现两个或多个对象之间相互引用，形成了一个环形结构时，就会将它们都标记为活动对象，不会被清理掉。同时，在后续的标记-清除过程中，如果回收机制发现某个对象被标记为垃圾，但它引用着其他的活动对象，那么就可以推断出这是一个循环引用，可以采取特殊的回收策略将其处理掉。

总之，在 Python 中，垃圾回收机制是通过引用计数和循环垃圾收集两种方式实现的，它们共同工作，确保内存空间被有效地回收和管理。



### with

在 Python 中，`with` 是一种上下文管理器，它可以在代码块开始之前执行一些操作，在结束时做一些清理工作。`with` 语句的语法格式如下：

```
pythonCopy Codewith context_expression [as target(s)]:
    with-block
```

其中 `context_expression` 表示一个上下文管理器对象，该对象必须实现 `__enter__()` 和 `__exit__()` 方法，这两个方法分别在进入和离开 `with` 块时被调用。 `as target(s)` 可选，表示将 `context_expression.__enter__()` 方法返回的值绑定到某个变量或元组上。

`with` 语句可以帮助程序员避免对资源的手动管理和释放，例如打开一个文件、建立一个数据库连接等。相应的代码可以写成如下形式：

```
pythonCopy Code# 1. 使用 with 打开文件
with open("hello.txt", "w") as f:
    f.write("Hello, World!")

# 2. 使用 with 建立数据库连接
import psycopg2

with psycopg2.connect(database="mydb", user="myuser", password="mypassword") as conn:
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM mytable")
```

在上述示例中，使用 `with` 打开文件时，当代码块执行结束时，Python 会自动关闭文件，无需手动调用 `close()` 方法；使用 `with` 建立数据库连接也类似，当程序执行完毕，Python 会自动关闭数据库连接。







### restfull和rpc区别与联系

RESTful 和 RPC（Remote Procedure Call）都是常用的网络通信协议，但它们有一些区别和联系。

首先，RESTful 是建立在 HTTP 协议之上的一种软件架构风格，它使用 URL 来标识资源，通过 HTTP 方法来对资源进行操作，包括 GET、POST、PUT、DELETE 等。RESTful 的设计思想是基于客户端和服务器之间的无状态通信，服务器不保存客户端的状态并且以资源为中心，充分利用 HTTP 的缓存、代理等机制，提高网络效率。RESTful 通常被用于 Web API 的开发，其良好的可读性和可扩展性使其逐渐成为 Web 应用程序的主要开发模式。

RPC 是远程过程调用协议，它允许程序在本地或远程系统上调用一个函数或方法，就像本地调用一样，屏蔽了底层通信细节，使得网络通信透明化、简化化。RPC 协议传输的是请求和响应的数据，可以通过多种传输协议实现，如 TCP、HTTP、UDP 等。RPC 协议具有更高的效率和更小的网络开销，因此多被用于传输大型数据和复杂的计算任务。

下面是 RESTful 和 RPC 的区别和联系：

1. RESTful 和 RPC 的通信方式不同。RESTful 是面向资源的，客户端使用 HTTP 方法（如 GET、POST、PUT、DELETE 等）来对服务器上的资源进行操作。RPC 是面向函数的，客户端通过调用远程函数来实现通信。
2. RESTful 和 RPC 的网络传输协议不同。RESTful 使用 HTTP 协议来传输数据，而 RPC 可以使用 TCP、HTTP、UDP 等多种协议来传输数据。
3. RESTful 和 RPC 的数据传输格式不同。RESTful 通常使用 JSON 或 XML 格式来传输数据，而 RPC 可以使用多种格式来传输数据，如二进制流、JSON、XML、Protobuf 等。
4. RESTful 和 RPC 的适用场景不同。RESTful 适用于大量资源的处理和 Web API 的开发，RPC 适用于需要高效传输数据和复杂计算任务时的应用场景。

综上所述，RESTful 和 RPC 都是常用的网络通信协议，它们在通信方式、网络传输协议、数据传输格式和适用场景等方面有区别和联系，具体选择哪种协议应根据实际情况来考虑。



### is 和 ==区别

在 Python 中，`is` 和 `==` 都是比较运算符，但它们的含义不同。

`is` 比较的是两个对象的身份标识，即它们**是否是同一个对象**。如果两个对象的身份标识相同，则返回 `True`，否则返回 `False`。

`==` 比较的是两个对象的**值是否相等**。如果两个对象的值相同，则返回 `True`，否则返回 `False`。

以下是示例代码，演示了 `is` 和 `==` 的不同：

```
pythonCopy Codea = [1, 2, 3] # 创建一个列表对象 a
b = [1, 2, 3] # 创建另一个列表对象 b

print(a == b) # 输出 True，因为 a 和 b 的值相等
print(a is b) # 输出 False，因为 a 和 b 是不同的对象

c = a # 创建一个对象 c，将其引用指向 a 指向的对象
print(a is c) # 输出 True，因为 a 和 c 引用同一个对象
```

需要注意的是，在 Python 中，一些常用的对象（如数值、字符串等）会被缓存起来供重用，因此可能会出现一些意外的结果。例如，对于整数值 `-5` 到 `256`，它们会被缓存起来，因此使用 `is` 比较同样的整数值时，会得到 `True` 的结果：

```
a = 1
b = 1
print(a is b) # 输出 True，因为 a 和 b 引用同一个整数对象
```



### 装饰器实现单例模式

```
def singleton(cls):
    instances = {}

    def getinstance(*args, **kwargs):
        if cls not in instances:
            instances[cls] = cls(*args, **kwargs)
        return instances[cls]
    return getinstance
    

@singleton
class MyClass:
    pass
```



### python内置的数据结构



Python 提供了多种内置的数据结构，以下是一些常用的数据结构：

列表（List）: 基于序列的动态数组，可以包含任意数量的元素，支持索引、切片、增加、删除等操作。
元组（Tuple）: 也是基于序列的不可变有序集合，可以包含任意数量的元素，一旦创建就不能修改。
集合（Set）: 无序的不重复元素的集合，可以进行交集、并集、差集等操作。
字典（Dictionary）: 一种映射类型，由键值对组成，支持通过键获取值，添加、删除、修改键值对等操作。
栈（Stack）: 后进先出的数据结构，只能从栈顶插入和删除元素。
队列（Queue）: 先进先出的数据结构，可以在队尾插入元素，在队头删除元素。
堆（Heap）: 一种特殊的树形数据结构，通常具有最小堆和最大堆两种形式。
链表（Linked List）: 通过指针连接各个节点的线性数据结构，可以分为单向链表、双向链表和循环链表。
以上是 Python 中常用的一些数据结构，不同的数据结构适用于不同的应用场景，掌握这些数据结构能够帮助我们更好地解决各种问题。



### 栈的使用

一个字符串，里面只有左括号和右括号元素，但是不确定都有多少，python如何实现判断是否里面的括号是闭合的，并计算最大深度

```
def max_depth(s):
    stack = []
    max_depth = 0
    depth = 0

    for c in s:
        if c == '(':
            stack.append(c)
            depth += 1
            if depth > max_depth:
                max_depth = depth
        elif c == ')':
            if not stack or stack[-1] != '(':
                return False
            stack.pop()
            depth -= 1

    return depth == 0, max_depth

在上述代码中，我们遍历字符串 s 中的每个字符，如果字符是左括号，则将其压入栈 stack 中，并更新当前深度 depth。如果当前深度大于最大深度，则更新最大深度。如果字符是右括号，则判定该字符是否与栈顶元素匹配，并同时更新当前深度。如果栈为空或者栈顶元素与当前右括号不匹配，则返回 False。最后，检查栈是否为空，如果为空说明该字符串是合法的闭合括号序列，返回 True 和最大深度；否则返回 False 和 0。
```



读取一个文件然后文件里面内容找合法的ip地址并并做去重

```
import re

# 读取文件
with open('file.txt', 'r') as f:
    s = f.read()

# 定义 IP 地址的正则表达式模式
pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'

# 使用正则表达式匹配 IP 地址，并存储到 set 集合中实现去重
ips = set(re.findall(pattern, s))

# 打印去重后的 IP 地址集合
print(ips)
```









字典按值排序（sorted(dict.iterms(), key=lambda x: x[1],）



翻转字符串  s = "w3423", s[::-1]



list1中age按由大到小排序（list1=[{"age": 5}, {"age": 67}, {"age": 56}],    sorted(list1, key=lambda x:x['age'], reverse=True)）



lista=[1,2,43,7],  lista[10:]



两个列表，找出相同元素和不同元素

list1= [1,2 ,4,6, 67],  list2=[3, 2, 43, 76]

set1=set(list1) , set2=set(list2), set1&set2,  set1^set2





### 什么是反射，反射应用场景

在 Python 中，反射是指程序可以访问、检查和修改其本身状态或行为的能力。通过反射，我们可以在运行时动态地获取对象的类型信息、属性、方法等，并且可以在不知道对象类型的情况下对其进行操作。

Python 提供了一些内置函数，例如 `getattr()`、`setattr()`、`hasattr()` 等，用于实现反射相关的功能。这些函数接收一个对象和一个属性名作为参数，然后进行相应的操作。例如，可以使用 `getattr(obj, "attr_name")` 来获取对象中的指定属性值，如果不存在则返回默认值；可以使用 `hasattr(obj, "attr_name")` 来检查对象是否存在指定的属性。

反射在 Python 中的应用场景比较广泛，下面列举几个典型的例子：

1. 动态导入模块：使用 `__import__(module_name)` 函数可以在运行时动态地导入指定模块，实现动态加载扩展功能的功能。
2. 插件架构：通过在程序运行时动态加载插件，实现程序功能的可扩展性。
3. 序列化和反序列化：当我们需要将对象序列化成字节流或 JSON 格式时，可以使用反射获取对象的属性列表和值列表，然后将它们转换为序列化格式；当我们需要从字节流或 JSON 中反序列化出对象时，可以使用反射创建对象，并为其设置属性值。
4. 测试框架：测试框架需要动态地加载测试用例，并对其进行执行和验证，可以使用反射实现动态加载并运行函数或方法。

总之，反射在 Python 中的应用非常广泛，可以大大提高程序的灵活性和可扩展性。但是，在使用反射时也需要注意一些安全性问题，例如可能会导致属性/方法访问不当、属性名字符串拼写错误等问题。因此，反射建议在必要时谨慎使用。





### 深拷贝和浅拷贝

浅拷贝会创建一个新的对象，但是这个新的对象还是会引用原始对象中的子对象。也就是说，浅拷贝只会复制原始对象的顶层结构，而其中的嵌套对象则会继续被共享。浅拷贝可以使用 `copy()` 方法或者切片来实现。

与之不同的是，深拷贝会创建一个新的对象，并递归地复制原始对象中所有的子对象。也就是说，深拷贝会完全独立于原始对象，不会共享其中的任何对象。深拷贝可以使用 `copy.deepcopy()` 方法来实现。





### `new和init`的区别



`__new__` 和 `__init__` 都是 Python 中用于创建对象的内置方法。它们在对象创建过程中具有不同的职责，因此二者之间具有一些区别。

`__new__` 是用于返回一个新的实例对象的方法，它是一个类级别的方法，在对象实例化之前调用。`__new__` 方法接收到的第一个参数是它所属的类，后续参数是用户传递的参数，返回值是创建的实例对象。在使用 `super().__new__(cls)` 时，它调用的是父类的 `__new__` 方法，从而创建了一个新的实例对象。

`__init__` 是用于初始化已经创建的实例对象的方法，它是一个实例级别的方法，在 `__new__` 方法返回实例对象之后调用。`__init__` 方法接收到的第一个参数是它所属的实例对象，后续参数是用户传递的参数。该方法不会返回任何值，它通过修改已经生成的对象的属性来对其进行初始化。







### GIL对python性能的影响

GIL（Global Interpreter Lock）是 Python 解释器中的一种机制，它的作用是确保同一时间只有一个线程在执行 Python 代码。这个机制是出于对解释器内部资源的保护和简化 C 拓展模块的设计而提出的。

虽然 GIL 的存在可以确保 Python 程序的安全性和稳定性，但同时也会对其性能产生一定的影响。由于 GIL 的存在，多线程的 Python 程序实际上是以并发的方式运行的，而不是真正的并行。因为在任何时刻都只有一个线程在执行 Python 代码，所以多个线程不能同时利用多核处理器的计算能力，从而限制了程序的性能和处理能力。

另外，由于 GIL 的存在，一个线程在持有解释器锁的时候，其他线程无法执行Python代码，只能等待当前线程释放锁。这就会导致CPU的利用率较低，从而降低程序的执行效率。

虽然 GIL 的存在是一种权衡，但是一些计算密集型任务、CPU 密集型的多线程程序以及高并发的网络应用程序等需要充分利用多核处理器的场景，都可能会受到 GIL 的限制，从而导致程序性能下降。针对这种情况，开发者可以考虑使用多进程、协程等替代方案，以便更好地发挥硬件的计算能力。





### 双下划线和单下划线

在 Python 中，双下划线和单下划线都有特殊的含义。

双下划线（__variable）表示私有变量，在类的内部指代使用，外部不可访问。在 Python 中，没有真正意义上的私有属性，但是使用双下划线可以让属性变成“伪私有”，因为 Python 解释器会将双下划线开头的变量名重命名，变成 _classname__variable 的形式，从而使得该变量在外部无法直接访问。例如，在一个类中定义了 __name 变量，在外部不能直接使用 obj.__name 访问，而应该使用 obj._classname__name 的方式来访问。需要注意的是，Python 中的私有变量只是一种约定俗成的规范，并不是强制性的。

单下划线（_variable）表示命名约定，表示该变量是“内部使用”的，不应该被外部直接访问。它主要用于区分临时变量和公共变量，也可以用于避免名称冲突。使用单下划线并不会改变变量的实际功能或访问权限，只是提醒开发者遵循良好的编程习惯。通常情况下，单下划线变量的功能与普通变量没有区别。

需要注意的是，单下划线和双下划线只是一种约定，不是关键字或保留字，因此在实际使用时需要注意遵循惯例。



with语句，如何构造，原理



单例模式，优缺点，如何实现







json序列化时，会遇到中文转unicode，想保留中文如何做（json.dumps({"dd", "你好"}， ensure_ascii=False)）



mro



C3算法



判断邮箱合法，re使用





python函数调用时候参数的传递是值传递还是引用传递







### python递归

#### 最大层数

在 Python 中，递归调用的最大深度由解释器的堆栈大小限制。默认情况下，Python 解释器限制递归深度不超过 1000 层。如果递归深度超过了这个值，会触发 `RecursionError` 异常，表示递归调用过深。

可以通过 `sys.setrecursionlimit()` 函数来修改递归调用的最大深度，但是不建议设置太大的值，因为过深的递归可能导致栈溢出等问题。

#### 停止条件

1. 达到指定的深度或层数：可以设置一个计数器或者参数来记录当前的递归层数或深度，当达到指定层数或深度时，不再继续调用自身，而是返回一个值或执行其他操作。
2. 达到特定的条件：可以根据具体的问题设置一些特定的条件，例如在查找树形结构中的某个节点时，当找到该节点时就不再继续递归。
3. 输入参数不符合要求：可以在函数开头对输入参数进行一些判断和筛选，当参数不符合要求时，直接返回一个错误信息或默认值。





### 列表推导式和生成器表达式

 输出结果分别是什么

```
[i % 2 for i in range(10)]    # 列表

(i % 2 for i in range(10))   # 迭代器
```



### 闭包

在 Python 中，闭包（Closure）是指一个函数对象和该函数所引用的外部变量组合而成的整体。

换句话说，闭包就是通过在一个函数内部定义另一个函数，并返回该函数对象，使得内部函数可以访问外部函数的变量和参数。这样做可以将函数和它所需的数据打包在一起，形成一个特殊的对象，具有不同于普通函数的行为。

常见的例子是利用闭包实现装饰器





### 并发

当需要实现高并发的 Python 应用程序时，可以使用 gevent 这个协程库来达到处理1000级以上并发的需求。下面是一些处理方法：

1. 使用协程：gevent 提供了基于协程的并发模型，可以在同一个线程中实现并发操作。使用 gevent 可以方便地创建和管理大量的协程，避免线程切换的开销。
2. 异步 I/O：利用 gevent 的协程特性，可以将阻塞式的 I/O 操作转换成异步非阻塞的方式。例如，可以使用 gevent 提供的 `gevent.socket`、`gevent.ssl` 等模块替代标准库中的阻塞式网络和加密操作，从而提高并发处理能力。
3. 批量处理：对于需要处理大量请求的场景，可以使用批量处理的方式来提高效率。将一批请求分组处理，利用协程的并发性质同时处理多个请求，减少请求间的等待时间。
4. 连接池和请求队列：为了更好地管理连接资源和控制请求的并发数量，可以使用连接池和请求队列来限制并发度。通过连接池可以复用连接资源，而请求队列可以控制同时处理的请求数量，避免资源竞争。
5. 响应超时处理：处理大量请求时，可能会遇到一些耗时较长的请求。为了避免这些请求对整体性能造成影响，可以设置合理的响应超时时间，并对超时的请求进行适当的处理。
6. 分布式部署：如果单台服务器无法满足处理大量并发的需求，可以考虑将应用程序进行分布式部署。使用负载均衡等方式将请求分发到多台服务器上，并利用 gevent 在每台服务器上处理并发请求。

需要注意的是，在高并发场景下，还需要对代码进行优化，避免性能瓶颈出现在其他方面，如数据库查询、文件操作等。可以通过使用缓存、异步任务、数据库连接池等技术来提升性能。

综上所述，通过使用 gevent 协程库以及结合其他优化手段，可以有效地处理1000级以上的并发请求



### 正则表达式 

`re.search(pattern, string, flags=0)`：在字符串 `string` 中搜索匹配正则表达式 `pattern` 的第一个位置，并返回匹配的结果对象。如果匹配成功，则返回 MatchObject 对象，否则返回 None。

```
import re

# 在字符串中查找是否包含字母 a
str = "Hello World!"
match_result = re.search(r'a', str)
if match_result:
    print("Match found: ", match_result.group())
else:
    print("Match not found")
```



`re.findall(pattern, string, flags=0)`：在字符串 `string` 中查找所有满足正则表达式 `pattern` 的非重叠匹配，并以列表形式返回匹配的结果。如果没有匹配成功，则返回空列表。



```
import re

# 查找字符串中所有数字
str = "The price of the book is $20.99 and the weight is 2.5kg"
num_list = re.findall(r'\d+', str)
print(num_list)
```



`re.match(pattern, string, flags=0)`：从字符串 `string` 的开头开始匹配正则表达式 `pattern`，如果匹配成功，则返回匹配的结果对象，否则返回 None。与 `re.search()` 不同的是，`re.match()` 必须从字符串的开头开始匹配。

示例代码：

```
import re

# 匹配字符串是否以 Hello 开头
str = "Hello World!"
match_result = re.match(r'Hello', str)
if match_result:
    print("Match found: ", match_result.group())
else:
    print("Match not found")
```



`re.compile(pattern, flags=0)` 方法用于将正则表达式模式 `pattern` 编译成一个正则表达式对象，并返回该对象。这个编译过程可以提高正则表达式的执行效率，因为编译后的正则表达式对象可以重复使用。

示例代码如下：

```
import re

pattern = r"\d+"  # 匹配任意数字
text = "hello 123 world 456"
regex = re.compile(pattern)  # 将正则表达式编译为正则对象

match_obj = regex.search(text)  # 直接调用正则对象的方法进行匹配

if match_obj:
    print("Match found: ", match_obj.group())
else:
    print("Match not found")
```



#### 读取一个文件，匹配文件中合法的ip地址（需要验证？）

```
import re

# 读取文件
with open('file.txt', 'r') as f:
    s = f.read()

# 定义 IP 地址的正则表达式模式 #结果不对
pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'

# 使用正则表达式匹配 IP 地址，并存储到 set 集合中实现去重
ips = set(re.findall(pattern, s))

# 打印去重后的 IP 地址集合
print(ips)


该正则表达式无法正确匹配符合 IPv4 标准的 IP 地址，原因是 [] 中的点号.只是一个普通字符，表示匹配任意一个字符。正确匹配 IPv4 地址的正则表达式应该是\b(?:\d{1,3}\.){3}\d{1,3}\b。

这个正则表达式具体含义如下：

\b 表示单词边界，避免匹配到 IP 地址的一部分。
(?:\d{1,3}\.){3} 表示匹配 3 个数字（每个数字长度为 1~3），并以点号.结尾的字符串，由于使用了非捕获分组 (?:)，所以匹配结果中不包含点号。
\d{1,3} 表示匹配一个长度为 1~3 的数字。
\b 表示单词边界，确保匹配到完整的 IP 地址。
```





### python面试题

https://juejin.cn/post/7067744227127459871





## 五、框架相关（django）



python三大框架各自应用场景



uWSGI 和nginx的理解

uwsgi  区别uWSGI



### 5.1 django



手动删除了表，导致，django迁移失败原因及解决办法

django中 Model    ForeignKey字段的on_delete参数作用



基于django使用ajax发送post请求时，有哪种方法携带csrftoken



django  FBV  CBV



django的request对象是什么时候创建的



django请求的生命周期



django中如何在model保存前做一定的固定操作，比如写一句日志（signal Dispatcher）



django中间件的使用



django  ORM查询中  select_related和prefetch_related的区别





cookie和session的区别





celery分布式队列





## 六、其他



### 自学python最大的困难

在自学 Python 的过程中，最大的挑战可能会是以下几个方面：

1. 缺乏指导和学习路径：自学意味着你需要自行策划学习的路线和计划。缺乏合适的指导和学习资源可能导致感到迷失或难以系统地学习 Python。

解决方法：寻找合适的学习资源，如在线教程、视频教程、书籍等，确保它们按照适合你的学习风格和能力水平来组织内容。可以参考官方文档、Python 相关的网站和社区，以及其他人的学习经验和建议。

1. 学习曲线陡峭：对于没有编程经验或没有接触过类似的技术的人来说，学习 Python 可能会有一定的难度。掌握基本概念、语法、数据结构和算法等需要时间和实践的努力。

解决方法：保持耐心和恒心，理解编程的基本原理，并通过实践编写代码来加深理解。找到合适的学习资源和练习项目，切勿急于求成，逐步提升自己的编程能力。

1. 缺乏实践经验：在学习 Python 的过程中，很容易陷入只看理论而缺乏实践的陷阱。没有实际项目或应用来应用你所学的知识，可能导致学习效果不佳。

解决方法：参与编程项目、练习和挑战，将你所学的知识应用到实际的情境中。可以尝试解决一些小问题、编写小型应用或参与开源项目，这样能够锻炼你的实践能力，并加深对 Python 的理解。

1. 缺乏反馈和指导：在自学的过程中，没有人对你的学习进展进行指导和反馈，可能会让你感到迷茫和孤立。

解决方法：积极参与编程社区和论坛，寻求他人的意见和帮助。在社交平台上寻找与你相似背景的学习伙伴或加入线上/线下的学习小组，互相交流和分享经验。此外，考虑寻找一位导师或参加编程课程，以获得更系统化的指导和反馈。

通过克服这些挑战，建立良好的学习习惯和坚持不懈的态度，你将能够克服自学中的困难，并逐渐掌握 Python 编程技能。祝你在学习 Python 的道路上取得成功！如果有任何其他问题，请随时提问。





工作中挑战哪一块最多的或者拿得出手的







这几年最有成就感的





计算、人工智能、机器学习

vue/java/go


  

